{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corpus ( NLP ).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragBhattacharjee17/Natural-Language-Processing/blob/master/Corpus_(_NLP_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnLYtVY8zfsT",
        "colab_type": "text"
      },
      "source": [
        "A collection of documents is referred to as a corpus. NLTK provides several corpora out of the box. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqu-tXity4z1",
        "colab_type": "code",
        "outputId": "570534e7-6947-4f2b-8d65-929813ff58c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "# brown corpus is a tagged corpus where each word in each file of the corpus is associated with its POS tag"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LJ2mON1zxAK",
        "colab_type": "code",
        "outputId": "6f759bea-4c9d-4ce1-968c-a07de76289bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Display all the files that are there in the corpus\n",
        "print(brown.fileids())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5pFh3xwz_6I",
        "colab_type": "code",
        "outputId": "b2d50640-1974-4573-94e5-414c219e2a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Display the paragraphs, sentences and words in a specific file of the corpus\n",
        "print(brown.paras('ca01'))\n",
        "print(brown.sents('ca02'))\n",
        "print(brown.words('ca03'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']], [['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']], ...]\n",
            "[['Austin', ',', 'Texas'], ['--', 'Committee', 'approval', 'of', 'Gov.', 'Price', \"Daniel's\", '``', 'abandoned', 'property', \"''\", 'act', 'seemed', 'certain', 'Thursday', 'despite', 'the', 'adamant', 'protests', 'of', 'Texas', 'bankers', '.'], ...]\n",
            "['Several', 'defendants', 'in', 'the', 'Summerdale', ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqaJABQn0XKT",
        "colab_type": "code",
        "outputId": "a59ca8c1-204e-4d69-d9b2-0909e458f337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Display the POS tag for each word in a specific file of the corpus\n",
        "print(brown.tagged_words('ca01'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5fG0YIP0nBs",
        "colab_type": "code",
        "outputId": "57fe70c9-f1aa-4a5e-9646-2c12a47736df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Loading the CoNLL2000 corpus which has 270k words that are tagged and chunked\n",
        "from nltk.corpus import conll2000\n",
        "nltk.download('conll2000')\n",
        "print(conll2000.fileids())\n",
        "print(conll2000.words('train.txt'))\n",
        "print(conll2000.chunked_sents('train.txt'))\n",
        "# When you print the chunked sentences, observe how chunks of tagged words are tagged.\n",
        "# For example, a chunk containing ('the' , 'DT') and ('pound', NN') is tagged as 'NP'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "['train.txt', 'test.txt']\n",
            "['Confidence', 'in', 'the', 'pound', 'is', 'widely', ...]\n",
            "[Tree('S', [Tree('NP', [('Confidence', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('the', 'DT'), ('pound', 'NN')]), Tree('VP', [('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB')]), Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]), ('if', 'IN'), Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]), Tree('PP', [('for', 'IN')]), Tree('NP', [('September', 'NNP')]), (',', ','), ('due', 'JJ'), Tree('PP', [('for', 'IN')]), Tree('NP', [('release', 'NN')]), Tree('NP', [('tomorrow', 'NN')]), (',', ','), Tree('VP', [('fail', 'VB'), ('to', 'TO'), ('show', 'VB')]), Tree('NP', [('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN')]), Tree('PP', [('from', 'IN')]), Tree('NP', [('July', 'NNP'), ('and', 'CC'), ('August', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS')]), ('.', '.')]), Tree('S', [('Chancellor', 'NNP'), Tree('PP', [('of', 'IN')]), Tree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), Tree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), Tree('PP', [('to', 'TO')]), Tree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), Tree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), Tree('NP', [('a', 'DT'), ('freefall', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('sterling', 'NN')]), Tree('PP', [('over', 'IN')]), Tree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')]), ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUjMdA2k6ZdO",
        "colab_type": "text"
      },
      "source": [
        "Consider 2 documents containing one sentence each, as follows\n",
        "\n",
        "Document 1: India is a republic country. We are proud Indians.\n",
        "\n",
        "Document 2: The current Prime Minister of India is Shri. Narendra Modi.\n",
        "\n",
        "In order to determine the similarity between these documents, we convert each of them into a vector of words and their counts.\n",
        "Suppose that we considered 2 documents are similar if the words occurring them are the same, then we just need to evaluate how many words are common in between the two documents. \n",
        "\n",
        "The matrix (table) that you see here is called as the Document Term Matrix (DTM) where each row represents a document and each column represents the term (token) in the document.\n",
        "\n",
        "Observe that while creating the DTM, some pre-processing is done, i.e, all words are converted to lower case and punctuation is removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCJtHOMZ9b9y",
        "colab_type": "text"
      },
      "source": [
        "**DTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spal72Gt6doy",
        "colab_type": "code",
        "outputId": "84a97116-b04b-4822-b4e6-37c85a03731e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# DocumentTermMatrix ( DTM )\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "sent1 = \"India is a republic country. We are proud Indians.\"\n",
        "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "dtm = count_vectorizer.fit_transform([sent1,sent2])\n",
        "print(pd.DataFrame(dtm.toarray(),columns=count_vectorizer.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   are  country  current  india  indians  ...  proud  republic  shri  the  we\n",
            "0    1        1        0      1        1  ...      1         1     0    0   1\n",
            "1    0        0        1      1        0  ...      0         0     1    1   0\n",
            "\n",
            "[2 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_RQYJRR8jCj",
        "colab_type": "code",
        "outputId": "e8620572-ba79-4391-a449-748f3732d85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Finding similarity between two documents using Euclidian Distance\n",
        "import scipy\n",
        "from scipy.spatial.distance import cosine\n",
        "print(cosine(dtm[0].toarray(),dtm[1].toarray()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7763932022500211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q0uykS99Zlp",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p-nyL2J9j_Q",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF is an abbreviation for Term Frequency - Inverse Document Frequency\n",
        "\n",
        "Here Term Frequency refers to the count of the occurrence of the term (token) within the document and Inverse Document Frequency refers to the count of the occurrence of a term across all documents.\n",
        "\n",
        "In this model, each document is represented as a vector of TF-IDFs. This model results in giving a lower weightage to words that commonly occur across the corpus (typically stop-words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMdRcFCF9xnL",
        "colab_type": "code",
        "outputId": "e92516bb-a53f-4f63-d406-1058e39270d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "sent1 = \"India is a republic country. We are proud Indians.\"\n",
        "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
        "\n",
        "tf_vectorizer = TfidfVectorizer()\n",
        "tf_idf = tf_vectorizer.fit_transform([sent1,sent2])\n",
        "print(pd.DataFrame(data=tf_idf.toarray(),columns=tf_vectorizer.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        are   country   current  ...      shri       the        we\n",
            "0  0.377628  0.377628  0.000000  ...  0.000000  0.000000  0.377628\n",
            "1  0.000000  0.000000  0.333102  ...  0.333102  0.333102  0.000000\n",
            "\n",
            "[2 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}